{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocA4IN5QX85s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN2navFN6s2m"
   },
   "source": [
    "#**Câu 1:**\n",
    "Hãy xây dựng mô hình logistic regression bằng tất cả các features trong file heart, so sánh với thư viện sklearn\n",
    "\n",
    "Bài này em tự code lại 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "FtU_G8StCDZd",
    "outputId": "6a6db055-30c5-4ad0-f093-2bf82753529f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
       "0     63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
       "1     37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
       "2     41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
       "3     56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
       "4     57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
       "..   ...  ...  ..       ...   ...  ...  ...    ...      ...    ...  ..   ...     ...\n",
       "298   57    0   0       140   241    0  ...      1      0.2      1   0     3       0\n",
       "299   45    1   3       110   264    0  ...      0      1.2      1   0     3       0\n",
       "300   68    1   0       144   193    1  ...      0      3.4      1   2     3       0\n",
       "301   57    1   0       130   131    0  ...      1      1.2      1   1     3       0\n",
       "302   57    0   1       130   236    0  ...      0      0.0      1   1     2       0\n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/huynhthanh98/ML/master/lab-03/heart.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD1fBu2IYCKo",
    "outputId": "b599fb18-2685-469b-8c5c-e348598ce6fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 63.    1.    3.  145.  233.    1.    0.  150.    0.    2.3   0.    0.\n",
      "    1. ]\n",
      " [ 37.    1.    2.  130.  250.    0.    1.  187.    0.    3.5   0.    0.\n",
      "    2. ]\n",
      " [ 41.    0.    1.  130.  204.    0.    0.  172.    0.    1.4   2.    0.\n",
      "    2. ]\n",
      " [ 56.    1.    1.  120.  236.    0.    1.  178.    0.    0.8   2.    0.\n",
      "    2. ]\n",
      " [ 57.    0.    0.  120.  354.    0.    1.  163.    1.    0.6   2.    0.\n",
      "    2. ]\n",
      " [ 57.    1.    0.  140.  192.    0.    1.  148.    0.    0.4   1.    0.\n",
      "    1. ]\n",
      " [ 56.    0.    1.  140.  294.    0.    0.  153.    0.    1.3   1.    0.\n",
      "    2. ]\n",
      " [ 44.    1.    1.  120.  263.    0.    1.  173.    0.    0.    2.    0.\n",
      "    3. ]\n",
      " [ 52.    1.    2.  172.  199.    1.    1.  162.    0.    0.5   2.    0.\n",
      "    3. ]\n",
      " [ 57.    1.    2.  150.  168.    0.    1.  174.    0.    1.6   2.    0.\n",
      "    2. ]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('target',axis = 1).values\n",
    "#X = df.loc[:,'chol'].values.reshape(df.shape[0],-1)\n",
    "y = df.loc[:,'target'].values.reshape(df.shape[0],-1)\n",
    "print(X[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K5s8DZXuYsYj",
    "outputId": "f539fd57-32ec-4073-c7e4-f3fe4822d739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46511628 1.         0.66666667 0.57142857 0.23325635 0.\n",
      "  0.         0.71962617 0.         0.28571429 1.         0.\n",
      "  1.        ]\n",
      " [0.55813953 0.         0.66666667 0.26530612 0.48267898 0.\n",
      "  0.5        0.78504673 0.         0.         1.         0.\n",
      "  0.66666667]\n",
      " [0.30232558 1.         0.         0.18367347 0.16859122 0.\n",
      "  0.5        0.51401869 0.         0.01785714 1.         0.\n",
      "  0.66666667]\n",
      " [0.55813953 1.         0.         0.20408163 0.43187067 0.\n",
      "  1.         0.48598131 0.         0.78571429 0.         1.\n",
      "  0.33333333]\n",
      " [0.55813953 1.         0.         0.06122449 0.23787529 0.\n",
      "  0.5        0.63551402 0.         0.01785714 1.         0.33333333\n",
      "  1.        ]\n",
      " [0.25581395 0.         0.33333333 0.36734694 0.23787529 0.\n",
      "  0.         0.81308411 0.         0.10714286 0.5        0.\n",
      "  0.66666667]\n",
      " [0.37209302 0.         0.33333333 0.26530612 0.26096998 0.\n",
      "  0.5        0.69158879 0.         0.19642857 1.         0.\n",
      "  0.66666667]\n",
      " [0.65116279 0.         0.         0.30612245 0.18013857 0.\n",
      "  0.5        0.70093458 0.         0.         1.         0.\n",
      "  0.66666667]\n",
      " [0.72093023 1.         0.         0.26530612 0.10623557 0.\n",
      "  0.5        0.48598131 0.         0.07142857 1.         0.\n",
      "  1.        ]\n",
      " [0.20930233 1.         0.         0.26530612 0.10623557 0.\n",
      "  0.         0.29906542 1.         0.44642857 0.5        0.\n",
      "  1.        ]]\n",
      "[[ 0.53488372  1.          0.          0.57142857  0.33487298  0.\n",
      "   0.          0.22429907  1.          0.10714286  0.5         0.33333333\n",
      "   0.33333333]\n",
      " [ 0.58139535  1.          1.          0.7755102   0.36258661  0.\n",
      "   0.          0.6635514   0.          0.03571429  0.5         0.\n",
      "   1.        ]\n",
      " [ 0.53488372  1.          0.66666667  0.57142857 -0.01154734  1.\n",
      "   0.5         0.79439252  0.          0.03571429  1.          0.33333333\n",
      "   1.        ]\n",
      " [ 0.51162791  0.          0.          0.40816327  0.64203233  0.\n",
      "   0.          0.57943925  1.          0.33928571  0.5         0.66666667\n",
      "   1.        ]\n",
      " [ 0.86046512  0.          0.66666667  0.16326531  0.30946882  1.\n",
      "   0.          0.39252336  0.          0.          1.          0.33333333\n",
      "   0.66666667]\n",
      " [ 0.53488372  1.          0.66666667  0.57142857  0.08545035  0.\n",
      "   0.5         0.80373832  0.          0.28571429  1.          0.\n",
      "   0.66666667]\n",
      " [ 0.27906977  0.          0.          0.44897959  0.25866051  0.\n",
      "   0.          0.59813084  1.          0.          0.5         0.\n",
      "   0.66666667]\n",
      " [ 0.51162791  0.          0.          1.08163265  0.36258661  1.\n",
      "   0.          0.42056075  1.          0.71428571  0.          0.66666667\n",
      "   1.        ]\n",
      " [ 0.46511628  1.          0.          0.28571429  0.35796767  0.\n",
      "   0.          0.26168224  1.          0.57142857  0.5         0.66666667\n",
      "   0.66666667]\n",
      " [ 0.46511628  1.          0.66666667  0.31632653  0.32794457  0.\n",
      "   0.          0.59813084  0.          0.08928571  0.          0.33333333\n",
      "   0.66666667]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "max = np.max(X_train,axis =0)\n",
    "min = np.min(X_train, axis=0)\n",
    "X_train = (X_train-min)/(max-min)\n",
    "X_test = (X_test-min)/(max-min)\n",
    "print(X_train[:10])\n",
    "print(X_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTJyNW3BZOq6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(X,W):\n",
    "  Z = np.matmul(X,W)\n",
    "  return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcHhIKXYa_j4"
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X,y):\n",
    "  lr = 0.0001\n",
    "  Xbar = np.concatenate([np.ones([X.shape[0],1]),X],axis = 1)\n",
    "  W = np.zeros([Xbar.shape[1],1])\n",
    "  bestLoss = 1e9\n",
    "  bestW = W\n",
    "  preLoss = 1e9\n",
    "  iStop = -1\n",
    "  for i in range(1000000):\n",
    "    #Z = f(Xbar*W) với f là hàm sigmoid\n",
    "    Z = sigmoid(Xbar,W).reshape(y.shape[0],-1)\n",
    "    error = np.matmul(Xbar.T,Z-y).reshape(W.shape[0],-1)\n",
    "    gradient = 1/Xbar.shape[0]*error\n",
    "    preShape = W.shape\n",
    "    W = W - lr*gradient\n",
    "    loss = np.mean(-y*np.log(Z)-(1-y)*np.log(1-Z))\n",
    "    if (i%1000 == 0):\n",
    "      print(\"Loss at epoch {}: {}\".format(i,loss))\n",
    "    # Cập nhật W tốt nhất\n",
    "    if (bestLoss>loss):\n",
    "      bestLoss = loss\n",
    "      bestW = W\n",
    "    # Thay đổi quá ít sau 1000 epoch thì thoát\n",
    "    if (abs(loss-preLoss)<1e-5): \n",
    "      if (i-iStop >= 1000):\n",
    "        break\n",
    "    else:\n",
    "      preLoss = loss\n",
    "      iStop = i\n",
    "  return bestW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6pjizQGg9qd",
    "outputId": "716e7ed6-f7f6-4fc2-ee95-d668a2eb16d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 0.6931471805599452\n",
      "Loss at epoch 1000: 0.68964080036551\n",
      "Loss at epoch 2000: 0.6861905299088603\n",
      "Loss at epoch 3000: 0.6827945207767044\n",
      "Loss at epoch 4000: 0.6794511198519207\n",
      "Loss at epoch 5000: 0.6761588374658\n",
      "Loss at epoch 6000: 0.6729163209376794\n",
      "Loss at epoch 7000: 0.6697223326018695\n",
      "Loss at epoch 8000: 0.6665757315695962\n",
      "Loss at epoch 9000: 0.6634754585977738\n",
      "Loss at epoch 10000: 0.6604205235404841\n",
      "Loss at epoch 11000: 0.6574099949461603\n",
      "Loss at epoch 12000: 0.6544429914363821\n",
      "Loss at epoch 13000: 0.651518674563121\n",
      "Loss at epoch 14000: 0.6486362428921462\n",
      "Loss at epoch 15000: 0.6457949271027799\n",
      "Loss at epoch 16000: 0.6429939859295917\n",
      "Loss at epoch 17000: 0.6402327028011338\n",
      "Loss at epoch 18000: 0.6375103830554035\n",
      "Loss at epoch 19000: 0.634826351632173\n",
      "Loss at epoch 20000: 0.6321799511593531\n",
      "Loss at epoch 21000: 0.6295705403647152\n",
      "Loss at epoch 22000: 0.6269974927560542\n",
      "Loss at epoch 23000: 0.6244601955226626\n",
      "Loss at epoch 24000: 0.6219580486190871\n",
      "Loss at epoch 25000: 0.6194904639988943\n",
      "Loss at epoch 26000: 0.6170568649717475\n",
      "Loss at epoch 27000: 0.6146566856617443\n",
      "Loss at epoch 28000: 0.6122893705488001\n",
      "Loss at epoch 29000: 0.6099543740780531\n",
      "Loss at epoch 30000: 0.6076511603249023\n",
      "Loss at epoch 31000: 0.6053792027054735\n",
      "Loss at epoch 32000: 0.603137983724117\n",
      "Loss at epoch 33000: 0.6009269947510352\n",
      "Loss at epoch 34000: 0.5987457358243746\n",
      "Loss at epoch 35000: 0.5965937154721375\n",
      "Loss at epoch 36000: 0.5944704505501125\n",
      "Loss at epoch 37000: 0.5923754660927186\n",
      "Loss at epoch 38000: 0.5903082951742312\n",
      "Loss at epoch 39000: 0.588268478778333\n",
      "Loss at epoch 40000: 0.5862555656743212\n",
      "Loss at epoch 41000: 0.5842691122986221\n",
      "Loss at epoch 42000: 0.5823086826405265\n",
      "Loss at epoch 43000: 0.5803738481312818\n",
      "Loss at epoch 44000: 0.5784641875358437\n",
      "Loss at epoch 45000: 0.5765792868467425\n",
      "Loss at epoch 46000: 0.5747187391796346\n",
      "Loss at epoch 47000: 0.5728821446702079\n",
      "Loss at epoch 48000: 0.5710691103721849\n",
      "Loss at epoch 49000: 0.5692792501562319\n",
      "Loss at epoch 50000: 0.5675121846096385\n",
      "Loss at epoch 51000: 0.565767540936662\n",
      "Loss at epoch 52000: 0.5640449528594783\n",
      "Loss at epoch 53000: 0.5623440605196921\n",
      "Loss at epoch 54000: 0.5606645103803918\n",
      "Loss at epoch 55000: 0.5590059551287443\n",
      "Loss at epoch 56000: 0.5573680535791367\n",
      "Loss at epoch 57000: 0.5557504705768842\n",
      "Loss at epoch 58000: 0.5541528769025299\n",
      "Loss at epoch 59000: 0.5525749491767636\n",
      "Loss at epoch 60000: 0.5510163697659939\n",
      "Loss at epoch 61000: 0.549476826688607\n",
      "Loss at epoch 62000: 0.5479560135219502\n",
      "Loss at epoch 63000: 0.5464536293100716\n",
      "Loss at epoch 64000: 0.5449693784722556\n",
      "Loss at epoch 65000: 0.5435029707123844\n",
      "Loss at epoch 66000: 0.5420541209291615\n",
      "Loss at epoch 67000: 0.5406225491272257\n",
      "Loss at epoch 68000: 0.5392079803291869\n",
      "Loss at epoch 69000: 0.5378101444886088\n",
      "Loss at epoch 70000: 0.5364287764039656\n",
      "Loss at epoch 71000: 0.5350636156335955\n",
      "Loss at epoch 72000: 0.5337144064116703\n",
      "Loss at epoch 73000: 0.5323808975652036\n",
      "Loss at epoch 74000: 0.5310628424321079\n",
      "Loss at epoch 75000: 0.5297599987803245\n",
      "Loss at epoch 76000: 0.5284721287280301\n",
      "Loss at epoch 77000: 0.5271989986649375\n",
      "Loss at epoch 78000: 0.5259403791746948\n",
      "Loss at epoch 79000: 0.5246960449583951\n",
      "Loss at epoch 80000: 0.5234657747591974\n",
      "Loss at epoch 81000: 0.5222493512880679\n",
      "Loss at epoch 82000: 0.5210465611506406\n",
      "Loss at epoch 83000: 0.5198571947751993\n",
      "Loss at epoch 84000: 0.5186810463417824\n",
      "Loss at epoch 85000: 0.5175179137124085\n",
      "Loss at epoch 86000: 0.5163675983624192\n",
      "Loss at epoch 87000: 0.515229905312937\n",
      "Loss at epoch 88000: 0.5141046430644333\n",
      "Loss at epoch 89000: 0.5129916235314\n",
      "Loss at epoch 90000: 0.5118906619781214\n",
      "Loss at epoch 91000: 0.5108015769555354\n",
      "Loss at epoch 92000: 0.5097241902391779\n",
      "Loss at epoch 93000: 0.5086583267682031\n",
      "Loss at epoch 94000: 0.5076038145854681\n",
      "Loss at epoch 95000: 0.5065604847786732\n",
      "Loss at epoch 96000: 0.5055281714225482\n",
      "Loss at epoch 97000: 0.5045067115220724\n",
      "Loss at epoch 98000: 0.5034959449567187\n",
      "Loss at epoch 99000: 0.5024957144257086\n",
      "Loss at epoch 100000: 0.5015058653942677\n",
      "Loss at epoch 101000: 0.5005262460408688\n",
      "Loss at epoch 102000: 0.49955670720544976\n",
      "Loss at epoch 103000: 0.49859710233859333\n",
      "Loss at epoch 104000: 0.49764728745165726\n",
      "Loss at epoch 105000: 0.49670712106783993\n",
      "Loss at epoch 106000: 0.49577646417416954\n",
      "Loss at epoch 107000: 0.49485518017440205\n",
      "Loss at epoch 108000: 0.49394313484281666\n",
      "Loss at epoch 109000: 0.4930401962788922\n",
      "Loss at epoch 110000: 0.49214623486285364\n",
      "Loss at epoch 111000: 0.4912611232120742\n",
      "Loss at epoch 112000: 0.49038473613831773\n",
      "Loss at epoch 113000: 0.4895169506058104\n",
      "Loss at epoch 114000: 0.48865764569012793\n",
      "Loss at epoch 115000: 0.4878067025378798\n",
      "Loss at epoch 116000: 0.4869640043271858\n",
      "Loss at epoch 117000: 0.4861294362289225\n",
      "Loss at epoch 118000: 0.48530288536873345\n",
      "Loss at epoch 119000: 0.48448424078978364\n",
      "Loss at epoch 120000: 0.48367339341625093\n",
      "Loss at epoch 121000: 0.48287023601753626\n",
      "Loss at epoch 122000: 0.4820746631731829\n",
      "Loss at epoch 123000: 0.48128657123849045\n",
      "Loss at epoch 124000: 0.48050585831081155\n",
      "Loss at epoch 125000: 0.47973242419651874\n",
      "Loss at epoch 126000: 0.4789661703786277\n",
      "Loss at epoch 127000: 0.478206999985067\n",
      "Loss at epoch 128000: 0.4774548177575789\n",
      "Loss at epoch 129000: 0.4767095300212424\n",
      "Loss at epoch 130000: 0.4759710446546056\n",
      "Loss at epoch 131000: 0.475239271060414\n",
      "Loss at epoch 132000: 0.474514120136927\n",
      "Loss at epoch 133000: 0.4737955042498051\n",
      "Loss at epoch 134000: 0.4730833372045646\n",
      "Loss at epoch 135000: 0.4723775342195815\n",
      "Loss at epoch 136000: 0.4716780118996394\n",
      "Loss at epoch 137000: 0.47098468821000417\n",
      "Loss at epoch 138000: 0.47029748245102365\n",
      "Loss at epoch 139000: 0.4696163152332333\n",
      "Loss at epoch 140000: 0.4689411084529634\n",
      "Loss at epoch 141000: 0.46827178526843527\n",
      "Loss at epoch 142000: 0.4676082700763381\n",
      "Loss at epoch 143000: 0.46695048848887394\n",
      "Loss at epoch 144000: 0.4662983673112653\n",
      "Loss at epoch 145000: 0.4656518345197139\n",
      "Loss at epoch 146000: 0.4650108192397991\n",
      "Loss at epoch 147000: 0.4643752517253132\n",
      "Loss at epoch 148000: 0.4637450633375173\n",
      "Loss at epoch 149000: 0.4631201865248155\n",
      "Loss at epoch 150000: 0.4625005548028347\n",
      "Loss at epoch 151000: 0.46188610273490305\n",
      "Loss at epoch 152000: 0.46127676591291955\n",
      "Loss at epoch 153000: 0.46067248093860574\n",
      "Loss at epoch 154000: 0.46007318540513237\n",
      "Loss at epoch 155000: 0.45947881787911127\n",
      "Loss at epoch 156000: 0.4588893178829487\n",
      "Loss at epoch 157000: 0.4583046258775494\n",
      "Loss at epoch 158000: 0.45772468324536375\n",
      "Loss at epoch 159000: 0.45714943227377497\n",
      "Loss at epoch 160000: 0.45657881613881507\n",
      "Loss at epoch 161000: 0.45601277888920366\n",
      "Loss at epoch 162000: 0.4554512654307044\n",
      "Loss at epoch 163000: 0.4548942215107923\n",
      "Loss at epoch 164000: 0.4543415937036223\n",
      "Loss at epoch 165000: 0.453793329395298\n",
      "Loss at epoch 166000: 0.4532493767694289\n",
      "Loss at epoch 167000: 0.4527096847929746\n",
      "Loss at epoch 168000: 0.4521742032023676\n",
      "Loss at epoch 169000: 0.45164288248990836\n",
      "Loss at epoch 170000: 0.45111567389042967\n",
      "Loss at epoch 171000: 0.45059252936822114\n",
      "Loss at epoch 172000: 0.4500734016042123\n",
      "Loss at epoch 173000: 0.4495582439834041\n",
      "Loss at epoch 174000: 0.44904701058254953\n",
      "Loss at epoch 175000: 0.4485396561580723\n",
      "Loss at epoch 176000: 0.44803613613422266\n",
      "Loss at epoch 177000: 0.4475364065914639\n",
      "Loss at epoch 178000: 0.44704042425508445\n",
      "Loss at epoch 179000: 0.4465481464840317\n",
      "Loss at epoch 180000: 0.4460595312599623\n",
      "Loss at epoch 181000: 0.4455745371765047\n",
      "Loss at epoch 182000: 0.44509312342872925\n",
      "Loss at epoch 183000: 0.444615249802822\n",
      "Loss at epoch 184000: 0.4441408766659577\n",
      "Loss at epoch 185000: 0.44366996495636774\n",
      "Loss at epoch 186000: 0.4432024761735983\n",
      "Loss at epoch 187000: 0.44273837236895763\n",
      "Loss at epoch 188000: 0.44227761613614397\n",
      "Loss at epoch 189000: 0.441820170602055\n",
      "Loss at epoch 190000: 0.4413659994177706\n",
      "Loss at epoch 191000: 0.4409150667497104\n",
      "Loss at epoch 192000: 0.4404673372709571\n",
      "Loss at epoch 193000: 0.4400227761527466\n",
      "Loss at epoch 194000: 0.4395813490561186\n",
      "Loss at epoch 195000: 0.4391430221237262\n",
      "Loss at epoch 196000: 0.43870776197179956\n",
      "Loss at epoch 197000: 0.43827553568226224\n",
      "Loss at epoch 198000: 0.43784631079499553\n",
      "Loss at epoch 199000: 0.4374200553002485\n",
      "Loss at epoch 200000: 0.43699673763119057\n",
      "Loss at epoch 201000: 0.4365763266566047\n",
      "Loss at epoch 202000: 0.43615879167371546\n",
      "Loss at epoch 203000: 0.43574410240115247\n",
      "Loss at epoch 204000: 0.4353322289720455\n",
      "Loss at epoch 205000: 0.434923141927247\n",
      "Loss at epoch 206000: 0.43451681220868027\n",
      "Loss at epoch 207000: 0.4341132111528138\n",
      "Loss at epoch 208000: 0.4337123104842522\n",
      "Loss at epoch 209000: 0.4333140823094494\n",
      "Loss at epoch 210000: 0.4329184991105357\n",
      "Loss at epoch 211000: 0.4325255337392595\n",
      "Loss at epoch 212000: 0.4321351594110395\n",
      "Loss at epoch 213000: 0.4317473496991267\n",
      "Loss at epoch 214000: 0.4313620785288733\n",
      "Loss at epoch 215000: 0.43097932017210616\n",
      "Loss at epoch 216000: 0.4305990492416023\n",
      "Loss at epoch 217000: 0.4302212406856668\n",
      "Loss at epoch 218000: 0.4298458697828068\n",
      "Loss at epoch 219000: 0.4294729121365045\n",
      "Loss at epoch 220000: 0.4291023436700826\n",
      "Loss at epoch 221000: 0.4287341406216645\n",
      "Loss at epoch 222000: 0.4283682795392233\n",
      "Loss at epoch 223000: 0.4280047372757225\n",
      "Loss at epoch 224000: 0.42764349098434135\n",
      "Loss at epoch 225000: 0.4272845181137877\n",
      "Loss at epoch 226000: 0.4269277964036938\n",
      "Loss at epoch 227000: 0.42657330388009457\n",
      "Loss at epoch 228000: 0.42622101885098634\n",
      "Loss at epoch 229000: 0.4258709199019649\n",
      "Loss at epoch 230000: 0.4255229858919409\n",
      "Loss at epoch 231000: 0.4251771959489307\n",
      "Loss at epoch 232000: 0.42483352946592257\n",
      "Loss at epoch 233000: 0.4244919660968143\n",
      "Loss at epoch 234000: 0.42415248575242503\n",
      "Loss at epoch 235000: 0.42381506859657325\n",
      "Loss at epoch 236000: 0.4234796950422282\n",
      "Loss at epoch 237000: 0.42314634574772475\n",
      "Loss at epoch 238000: 0.4228150016130464\n",
      "Loss at epoch 239000: 0.42248564377617276\n",
      "Loss at epoch 240000: 0.4221582536094906\n",
      "Loss at epoch 241000: 0.4218328127162668\n",
      "Loss at epoch 242000: 0.4215093029271833\n",
      "Loss at epoch 243000: 0.4211877062969308\n",
      "Loss at epoch 244000: 0.4208680051008627\n",
      "Loss at epoch 245000: 0.420550181831705\n",
      "Loss at epoch 246000: 0.4202342191963243\n",
      "Loss at epoch 247000: 0.4199201001125503\n",
      "Loss at epoch 248000: 0.4196078077060533\n",
      "Loss at epoch 249000: 0.4192973253072748\n",
      "Loss at epoch 250000: 0.4189886364484104\n",
      "Loss at epoch 251000: 0.4186817248604433\n",
      "Loss at epoch 252000: 0.41837657447023024\n",
      "Loss at epoch 253000: 0.4180731693976347\n",
      "Loss at epoch 254000: 0.4177714939527098\n",
      "Loss at epoch 255000: 0.41747153263292763\n",
      "Loss at epoch 256000: 0.4171732701204574\n",
      "Loss at epoch 257000: 0.4168766912794867\n",
      "Loss at epoch 258000: 0.4165817811535896\n",
      "Loss at epoch 259000: 0.41628852496313806\n",
      "Loss at epoch 260000: 0.41599690810275625\n",
      "Loss at epoch 261000: 0.4157069161388186\n",
      "Loss at epoch 262000: 0.415418534806988\n",
      "Loss at epoch 263000: 0.41513175000979574\n",
      "Loss at epoch 264000: 0.41484654781426084\n",
      "Loss at epoch 265000: 0.41456291444955057\n",
      "Loss at epoch 266000: 0.4142808363046766\n",
      "Loss at epoch 267000: 0.4140002999262314\n",
      "Loss at epoch 268000: 0.41372129201616187\n",
      "Loss at epoch 269000: 0.413443799429578\n",
      "Loss at epoch 270000: 0.41316780917259827\n",
      "Loss at epoch 271000: 0.4128933084002298\n",
      "Loss at epoch 272000: 0.4126202844142851\n",
      "Loss at epoch 273000: 0.41234872466132877\n",
      "Loss at epoch 274000: 0.4120786167306609\n",
      "Loss at epoch 275000: 0.4118099483523324\n",
      "Loss at epoch 276000: 0.4115427073951914\n",
      "Loss at epoch 277000: 0.41127688186496164\n",
      "Loss at epoch 278000: 0.4110124599023522\n",
      "Loss at epoch 279000: 0.41074942978119816\n",
      "Loss at epoch 280000: 0.41048777990662905\n",
      "Loss at epoch 281000: 0.410227498813268\n",
      "Loss at epoch 282000: 0.4099685751634601\n",
      "Loss at epoch 283000: 0.40971099774552827\n",
      "Loss at epoch 284000: 0.40945475547205623\n",
      "Loss at epoch 285000: 0.40919983737820004\n",
      "Loss at epoch 286000: 0.40894623262002566\n",
      "Loss at epoch 287000: 0.408693930472873\n",
      "Loss at epoch 288000: 0.4084429203297451\n",
      "Loss at epoch 289000: 0.4081931916997242\n",
      "Loss at epoch 290000: 0.40794473420641103\n",
      "Loss at epoch 291000: 0.40769753758638894\n",
      "Loss at epoch 292000: 0.4074515916877135\n",
      "Loss at epoch 293000: 0.40720688646842335\n",
      "Loss at epoch 294000: 0.4069634119950759\n",
      "Loss at epoch 295000: 0.40672115844130585\n",
      "Loss at epoch 296000: 0.40648011608640444\n",
      "Loss at epoch 297000: 0.40624027531392287\n",
      "Loss at epoch 298000: 0.4060016266102957\n",
      "Loss at epoch 299000: 0.4057641605634856\n",
      "Loss at epoch 300000: 0.40552786786165074\n",
      "Loss at epoch 301000: 0.40529273929182946\n",
      "Loss at epoch 302000: 0.4050587657386485\n",
      "Loss at epoch 303000: 0.4048259381830484\n",
      "Loss at epoch 304000: 0.40459424770102936\n",
      "Loss at epoch 305000: 0.404363685462417\n",
      "Loss at epoch 306000: 0.4041342427296451\n",
      "Loss at epoch 307000: 0.4039059108565578\n",
      "Loss at epoch 308000: 0.40367868128723133\n",
      "Loss at epoch 309000: 0.40345254555480997\n",
      "Loss at epoch 310000: 0.4032274952803632\n",
      "Loss at epoch 311000: 0.40300352217175794\n",
      "Loss at epoch 312000: 0.4027806180225481\n",
      "Loss at epoch 313000: 0.4025587747108817\n",
      "Loss at epoch 314000: 0.40233798419842326\n",
      "Loss at epoch 315000: 0.40211823852929185\n",
      "Loss at epoch 316000: 0.40189952982901744\n",
      "Loss at epoch 317000: 0.4016818503035093\n",
      "Loss at epoch 318000: 0.40146519223804245\n",
      "Loss at epoch 319000: 0.4012495479962579\n",
      "Loss at epoch 320000: 0.40103491001917724\n",
      "Loss at epoch 321000: 0.400821270824234\n",
      "Loss at epoch 322000: 0.40060862300431577\n",
      "Loss at epoch 323000: 0.40039695922682295\n",
      "Loss at epoch 324000: 0.40018627223274145\n",
      "Loss at epoch 325000: 0.39997655483572686\n",
      "Loss at epoch 326000: 0.3997677999212038\n",
      "Loss at epoch 327000: 0.3995600004454782\n",
      "Loss at epoch 328000: 0.3993531494348609\n",
      "Loss at epoch 329000: 0.3991472399848065\n",
      "Loss at epoch 330000: 0.39894226525906246\n",
      "Loss at epoch 331000: 0.39873821848883184\n",
      "Loss at epoch 332000: 0.39853509297194745\n",
      "Loss at epoch 333000: 0.3983328820720578\n",
      "Loss at epoch 334000: 0.39813157921782566\n",
      "Loss at epoch 335000: 0.39793117790213656\n",
      "Loss at epoch 336000: 0.39773167168132095\n",
      "Loss at epoch 337000: 0.397533054174384\n",
      "Loss at epoch 338000: 0.3973353190622502\n",
      "Loss at epoch 339000: 0.39713846008701603\n",
      "Loss at epoch 340000: 0.3969424710512144\n",
      "Loss at epoch 341000: 0.3967473458170889\n",
      "Loss at epoch 342000: 0.396553078305879\n",
      "Loss at epoch 343000: 0.3963596624971148\n",
      "Loss at epoch 344000: 0.39616709242792125\n",
      "Loss at epoch 345000: 0.39597536219233387\n",
      "Loss at epoch 346000: 0.3957844659406223\n",
      "Loss at epoch 347000: 0.3955943978786239\n",
      "Loss at epoch 348000: 0.395405152267087\n",
      "Loss at epoch 349000: 0.39521672342102254\n",
      "Loss at epoch 350000: 0.3950291057090658\n",
      "Loss at epoch 351000: 0.39484229355284634\n",
      "Loss at epoch 352000: 0.3946562814263663\n",
      "Loss at epoch 353000: 0.3944710638553876\n",
      "Loss at epoch 354000: 0.39428663541682896\n",
      "Loss at epoch 355000: 0.39410299073816807\n",
      "Loss at epoch 356000: 0.393920124496856\n",
      "Loss at epoch 357000: 0.39373803141973573\n",
      "Loss at epoch 358000: 0.39355670628247136\n",
      "Loss at epoch 359000: 0.3933761439089842\n",
      "Loss at epoch 360000: 0.3931963391708963\n",
      "Loss at epoch 361000: 0.393017286986982\n",
      "Loss at epoch 362000: 0.3928389823226271\n",
      "Loss at epoch 363000: 0.3926614201892942\n",
      "Loss at epoch 364000: 0.3924845956439962\n",
      "Loss at epoch 365000: 0.39230850378877763\n",
      "Loss at epoch 366000: 0.3921331397702016\n",
      "Loss at epoch 367000: 0.3919584987788442\n",
      "Loss at epoch 368000: 0.39178457604879535\n",
      "Loss at epoch 369000: 0.39161136685716735\n",
      "Loss at epoch 370000: 0.3914388665236093\n",
      "Loss at epoch 371000: 0.3912670704098273\n",
      "Loss at epoch 372000: 0.39109597391911244\n",
      "Loss at epoch 373000: 0.39092557249587384\n",
      "Loss at epoch 374000: 0.39075586162517945\n",
      "Loss at epoch 375000: 0.3905868368323008\n",
      "Loss at epoch 376000: 0.3904184936822656\n",
      "Loss at epoch 377000: 0.3902508277794152\n",
      "Loss at epoch 378000: 0.3900838347669688\n",
      "Loss at epoch 379000: 0.3899175103265918\n",
      "Loss at epoch 380000: 0.38975185017797154\n",
      "Loss at epoch 381000: 0.3895868500783981\n",
      "Loss at epoch 382000: 0.38942250582234983\n",
      "Loss at epoch 383000: 0.3892588132410848\n",
      "Loss at epoch 384000: 0.3890957682022384\n",
      "Loss at epoch 385000: 0.3889333666094239\n",
      "Loss at epoch 386000: 0.3887716044018412\n",
      "Loss at epoch 387000: 0.3886104775538881\n",
      "Loss at epoch 388000: 0.38844998207477827\n",
      "Loss at epoch 389000: 0.3882901140081626\n",
      "Loss at epoch 390000: 0.388130869431757\n",
      "Loss at epoch 391000: 0.387972244456974\n",
      "Loss at epoch 392000: 0.38781423522855957\n",
      "Loss at epoch 393000: 0.3876568379242339\n",
      "Loss at epoch 394000: 0.38750004875433786\n",
      "Loss at epoch 395000: 0.3873438639614829\n",
      "Loss at epoch 396000: 0.3871882798202058\n",
      "Loss at epoch 397000: 0.3870332926366281\n",
      "Loss at epoch 398000: 0.3868788987481199\n",
      "Loss at epoch 399000: 0.38672509452296633\n",
      "Loss at epoch 400000: 0.38657187636004126\n",
      "Loss at epoch 401000: 0.38641924068848227\n",
      "Loss at epoch 402000: 0.38626718396737075\n",
      "Loss at epoch 403000: 0.386115702685417\n",
      "Loss at epoch 404000: 0.3859647933606474\n",
      "Loss at epoch 405000: 0.3858144525400969\n",
      "Loss at epoch 406000: 0.3856646767995051\n",
      "Loss at epoch 407000: 0.38551546274301535\n",
      "Loss at epoch 408000: 0.38536680700287884\n",
      "Loss at epoch 409000: 0.3852187062391611\n",
      "Loss at epoch 410000: 0.3850711571394535\n",
      "Loss at epoch 411000: 0.3849241564185864\n",
      "Loss at epoch 412000: 0.3847777008183486\n",
      "Loss at epoch 413000: 0.3846317871072068\n",
      "Loss at epoch 414000: 0.3844864120800318\n",
      "Loss at epoch 415000: 0.3843415725578262\n",
      "Loss at epoch 416000: 0.3841972653874558\n",
      "Loss at epoch 417000: 0.38405348744138396\n",
      "Loss at epoch 418000: 0.3839102356174107\n",
      "Loss at epoch 419000: 0.3837675068384123\n",
      "Loss at epoch 420000: 0.38362529805208706\n",
      "Loss at epoch 421000: 0.3834836062307023\n",
      "Loss at epoch 422000: 0.38334242837084415\n",
      "Loss at epoch 423000: 0.38320176149317264\n",
      "Loss at epoch 424000: 0.38306160264217637\n",
      "Loss at epoch 425000: 0.3829219488859333\n",
      "Loss at epoch 426000: 0.38278279731587234\n",
      "Loss at epoch 427000: 0.3826441450465389\n",
      "Loss at epoch 428000: 0.38250598921536305\n",
      "Loss at epoch 429000: 0.38236832698242945\n",
      "Loss at epoch 430000: 0.3822311555302523\n",
      "Loss at epoch 431000: 0.3820944720635499\n",
      "Loss at epoch 432000: 0.381958273809025\n",
      "Loss at epoch 433000: 0.3818225580151452\n",
      "Loss at epoch 434000: 0.3816873219519283\n",
      "Loss at epoch 435000: 0.38155256291072753\n",
      "Loss at epoch 436000: 0.3814182782040212\n",
      "Loss at epoch 437000: 0.3812844651652055\n",
      "Loss at epoch 438000: 0.38115112114838695\n",
      "Loss at epoch 439000: 0.38101824352817976\n",
      "Loss at epoch 440000: 0.3808858296995053\n",
      "Loss at epoch 441000: 0.3807538770773921\n",
      "Loss at epoch 442000: 0.3806223830967806\n",
      "Loss at epoch 443000: 0.3804913452123283\n",
      "Loss at epoch 444000: 0.3803607608982188\n",
      "Loss at epoch 445000: 0.38023062764797094\n",
      "Loss at epoch 446000: 0.3801009429742524\n",
      "Loss at epoch 447000: 0.37997170440869416\n",
      "Loss at epoch 448000: 0.37984290950170735\n",
      "Loss at epoch 449000: 0.37971455582230257\n",
      "Loss at epoch 450000: 0.37958664095791045\n",
      "Loss at epoch 451000: 0.37945916251420514\n",
      "Loss at epoch 452000: 0.37933211811492984\n",
      "Loss at epoch 453000: 0.3792055054017234\n",
      "Loss at epoch 454000: 0.3790793220339496\n",
      "Loss at epoch 455000: 0.3789535656885292\n",
      "Loss at epoch 456000: 0.3788282340597713\n",
      "Loss at epoch 457000: 0.37870332485921004\n",
      "Loss at epoch 458000: 0.37857883581544005\n",
      "Loss at epoch 459000: 0.37845476467395606\n",
      "Loss at epoch 460000: 0.378331109196993\n",
      "Loss at epoch 461000: 0.37820786716336874\n",
      "Loss at epoch 462000: 0.3780850363683274\n",
      "Loss at epoch 463000: 0.37796261462338576\n",
      "Loss at epoch 464000: 0.3778405997561807\n",
      "Loss at epoch 465000: 0.3777189896103189\n",
      "Loss at epoch 466000: 0.37759778204522704\n",
      "Loss at epoch 467000: 0.3774769749360048\n",
      "Loss at epoch 468000: 0.3773565661732799\n",
      "Loss at epoch 469000: 0.3772365536630625\n",
      "Loss at epoch 470000: 0.37711693532660423\n",
      "Loss at epoch 471000: 0.37699770910025604\n",
      "Loss at epoch 472000: 0.37687887293533007\n",
      "Loss at epoch 473000: 0.37676042479796074\n",
      "Loss at epoch 474000: 0.37664236266896917\n",
      "Loss at epoch 475000: 0.3765246845437285\n",
      "Loss at epoch 476000: 0.3764073884320299\n",
      "Loss at epoch 477000: 0.3762904723579517\n",
      "Loss at epoch 478000: 0.37617393435972785\n",
      "Loss at epoch 479000: 0.3760577724896205\n",
      "Loss at epoch 480000: 0.37594198481379076\n",
      "Loss at epoch 481000: 0.375826569412174\n",
      "Loss at epoch 482000: 0.37571152437835453\n",
      "Loss at epoch 483000: 0.37559684781944164\n",
      "Loss at epoch 484000: 0.3754825378559486\n",
      "Loss at epoch 485000: 0.3753685926216714\n",
      "Loss at epoch 486000: 0.3752550102635686\n",
      "Loss at epoch 487000: 0.37514178894164507\n",
      "Loss at epoch 488000: 0.37502892682883265\n",
      "Loss at epoch 489000: 0.3749164221108768\n",
      "Loss at epoch 490000: 0.37480427298622077\n",
      "Loss at epoch 491000: 0.374692477665893\n",
      "Loss at epoch 492000: 0.37458103437339535\n",
      "Loss at epoch 493000: 0.37446994134459166\n",
      "Loss at epoch 494000: 0.37435919682759905\n",
      "Loss at epoch 495000: 0.37424879908267905\n",
      "Loss at epoch 496000: 0.3741387463821305\n",
      "Loss at epoch 497000: 0.3740290370101837\n",
      "Loss at epoch 498000: 0.3739196692628947\n",
      "Loss at epoch 499000: 0.3738106414480425\n",
      "Loss at epoch 500000: 0.373701951885025\n",
      "Loss at epoch 501000: 0.3735935989047588\n",
      "Loss at epoch 502000: 0.3734855808495774\n",
      "Loss at epoch 503000: 0.3733778960731321\n",
      "Loss at epoch 504000: 0.37327054294029366\n",
      "Loss at epoch 505000: 0.37316351982705454\n",
      "Loss at epoch 506000: 0.37305682512043264\n",
      "Loss at epoch 507000: 0.3729504572183762\n",
      "Loss at epoch 508000: 0.3728444145296682\n",
      "Loss at epoch 509000: 0.37273869547383404\n",
      "Loss at epoch 510000: 0.37263329848104876\n",
      "Loss at epoch 511000: 0.37252822199204527\n",
      "Loss at epoch 512000: 0.3724234644580238\n",
      "Loss at epoch 513000: 0.37231902434056285\n",
      "Loss at epoch 514000: 0.3722149001115293\n",
      "Loss at epoch 515000: 0.372111090252992\n",
      "Loss at epoch 516000: 0.3720075932571333\n",
      "Loss at epoch 517000: 0.37190440762616445\n",
      "Loss at epoch 518000: 0.37180153187223997\n",
      "Loss at epoch 519000: 0.371698964517373\n",
      "Loss at epoch 520000: 0.37159670409335244\n",
      "Loss at epoch 521000: 0.37149474914166014\n",
      "Loss at epoch 522000: 0.3713930982133893\n",
      "Loss at epoch 523000: 0.37129174986916325\n",
      "Loss at epoch 524000: 0.371190702679056\n",
      "Loss at epoch 525000: 0.3710899552225128\n",
      "Loss at epoch 526000: 0.3709895060882713\n",
      "Loss at epoch 527000: 0.37088935387428423\n",
      "Loss at epoch 528000: 0.3707894971876426\n",
      "Loss at epoch 529000: 0.37068993464449945\n",
      "Loss at epoch 530000: 0.37059066486999476\n",
      "Loss at epoch 531000: 0.3704916864981803\n",
      "Loss at epoch 532000: 0.3703929981719466\n",
      "Loss at epoch 533000: 0.37029459854294927\n",
      "Loss at epoch 534000: 0.3701964862715369\n",
      "Loss at epoch 535000: 0.3700986600266797\n",
      "Loss at epoch 536000: 0.3700011184858976\n",
      "Loss at epoch 537000: 0.3699038603351913\n",
      "Loss at epoch 538000: 0.36980688426897196\n",
      "Loss at epoch 539000: 0.36971018898999214\n",
      "Loss at epoch 540000: 0.3696137732092789\n",
      "Loss at epoch 541000: 0.369517635646065\n",
      "Loss at epoch 542000: 0.36942177502772244\n",
      "Loss at epoch 543000: 0.36932619008969697\n",
      "Loss at epoch 544000: 0.36923087957544154\n",
      "Loss at epoch 545000: 0.3691358422363527\n",
      "Loss at epoch 546000: 0.36904107683170445\n",
      "Loss at epoch 547000: 0.3689465821285873\n",
      "Loss at epoch 548000: 0.3688523569018428\n",
      "Loss at epoch 549000: 0.3687583999340025\n",
      "Loss at epoch 550000: 0.3686647100152262\n",
      "Loss at epoch 551000: 0.3685712859432403\n",
      "Loss at epoch 552000: 0.3684781265232772\n",
      "Loss at epoch 553000: 0.36838523056801603\n",
      "Loss at epoch 554000: 0.3682925968975227\n",
      "Loss at epoch 555000: 0.368200224339191\n",
      "Loss at epoch 556000: 0.36810811172768465\n",
      "Loss at epoch 557000: 0.3680162579048794\n",
      "Loss at epoch 558000: 0.3679246617198062\n",
      "Loss at epoch 559000: 0.3678333220285941\n",
      "Loss at epoch 560000: 0.3677422376944148\n",
      "Loss at epoch 561000: 0.36765140758742665\n",
      "Loss at epoch 562000: 0.3675608305847199\n",
      "Loss at epoch 563000: 0.367470505570262\n",
      "Loss at epoch 564000: 0.3673804314348442\n",
      "Loss at epoch 565000: 0.36729060707602745\n",
      "Loss at epoch 566000: 0.36720103139809\n",
      "Loss at epoch 567000: 0.36711170331197457\n",
      "Loss at epoch 568000: 0.367022621735237\n",
      "Loss at epoch 569000: 0.36693378559199413\n",
      "Loss at epoch 570000: 0.3668451938128732\n",
      "Loss at epoch 571000: 0.3667568453349616\n",
      "Loss at epoch 572000: 0.36666873910175624\n",
      "Loss at epoch 573000: 0.366580874063115\n",
      "Loss at epoch 574000: 0.3664932491752064\n",
      "Loss at epoch 575000: 0.36640586340046216\n",
      "Loss at epoch 576000: 0.3663187157075286\n",
      "Loss at epoch 577000: 0.36623180507121844\n",
      "Loss at epoch 578000: 0.36614513047246455\n",
      "Loss at epoch 579000: 0.36605869089827225\n",
      "Loss at epoch 580000: 0.36597248534167354\n",
      "Loss at epoch 581000: 0.3658865128016807\n",
      "Loss at epoch 582000: 0.36580077228324115\n",
      "Loss at epoch 583000: 0.3657152627971918\n",
      "Loss at epoch 584000: 0.36562998336021496\n",
      "Loss at epoch 585000: 0.3655449329947941\n",
      "Loss at epoch 586000: 0.3654601107291692\n",
      "Loss at epoch 587000: 0.36537551559729364\n",
      "Loss at epoch 588000: 0.3652911466387919\n",
      "Loss at epoch 589000: 0.36520700289891583\n",
      "Loss at epoch 590000: 0.36512308342850297\n",
      "Loss at epoch 591000: 0.3650393872839342\n",
      "Loss at epoch 592000: 0.3649559135270926\n",
      "Loss at epoch 593000: 0.3648726612253223\n",
      "Loss at epoch 594000: 0.36478962945138754\n",
      "Loss at epoch 595000: 0.36470681728343235\n",
      "Loss at epoch 596000: 0.36462422380494025\n",
      "Loss at epoch 597000: 0.3645418481046952\n",
      "Loss at epoch 598000: 0.3644596892767417\n",
      "Loss at epoch 599000: 0.3643777464203464\n",
      "Loss at epoch 600000: 0.3642960186399587\n",
      "Loss at epoch 601000: 0.3642145050451729\n",
      "Loss at epoch 602000: 0.364133204750691\n",
      "Loss at epoch 603000: 0.36405211687628375\n",
      "Loss at epoch 604000: 0.3639712405467543\n",
      "Loss at epoch 605000: 0.36389057489190124\n",
      "Loss at epoch 606000: 0.36381011904648175\n",
      "Loss at epoch 607000: 0.3637298721501758\n",
      "Loss at epoch 608000: 0.3636498333475493\n",
      "Loss at epoch 609000: 0.3635700017880194\n",
      "Loss at epoch 610000: 0.36349037662581885\n",
      "Loss at epoch 611000: 0.3634109570199612\n",
      "Loss at epoch 612000: 0.3633317421342059\n",
      "Loss at epoch 613000: 0.3632527311370239\n",
      "Loss at epoch 614000: 0.36317392320156355\n",
      "Loss at epoch 615000: 0.3630953175056176\n",
      "Loss at epoch 616000: 0.363016913231588\n",
      "Loss at epoch 617000: 0.36293870956645496\n",
      "Loss at epoch 618000: 0.36286070570174234\n",
      "Loss at epoch 619000: 0.3627829008334854\n",
      "Loss at epoch 620000: 0.3627052941621995\n",
      "Loss at epoch 621000: 0.3626278848928466\n",
      "Loss at epoch 622000: 0.3625506722348043\n",
      "Loss at epoch 623000: 0.36247365540183485\n",
      "Loss at epoch 624000: 0.3623968336120525\n",
      "Loss at epoch 625000: 0.3623202060878942\n",
      "Loss at epoch 626000: 0.36224377205608765\n",
      "Loss at epoch 627000: 0.3621675307476215\n",
      "Loss at epoch 628000: 0.362091481397715\n",
      "Loss at epoch 629000: 0.3620156232457889\n",
      "Loss at epoch 630000: 0.36193995553543445\n",
      "Loss at epoch 631000: 0.361864477514385\n",
      "Loss at epoch 632000: 0.3617891884344867\n",
      "Loss at epoch 633000: 0.36171408755166934\n",
      "Loss at epoch 634000: 0.36163917412591834\n",
      "Loss at epoch 635000: 0.3615644474212456\n",
      "Loss at epoch 636000: 0.36148990670566183\n",
      "Loss at epoch 637000: 0.361415551251149\n",
      "Loss at epoch 638000: 0.3613413803336322\n",
      "Loss at epoch 639000: 0.36126739323295176\n",
      "Loss at epoch 640000: 0.36119358923283684\n",
      "Loss at epoch 641000: 0.36111996762087845\n",
      "Loss at epoch 642000: 0.36104652768850226\n",
      "Loss at epoch 643000: 0.3609732687309425\n",
      "Loss at epoch 644000: 0.36090019004721535\n",
      "Loss at epoch 645000: 0.36082729094009297\n",
      "Loss at epoch 646000: 0.360754570716078\n",
      "Loss at epoch 647000: 0.36068202868537763\n",
      "Loss at epoch 648000: 0.3606096641618776\n",
      "Loss at epoch 649000: 0.3605374764631185\n",
      "Loss at epoch 650000: 0.36046546491026943\n",
      "Loss at epoch 651000: 0.3603936288281038\n",
      "Loss at epoch 652000: 0.3603219675449743\n",
      "Loss at epoch 653000: 0.3602504803927896\n",
      "Loss at epoch 654000: 0.36017916670698874\n",
      "Loss at epoch 655000: 0.36010802582651824\n",
      "Loss at epoch 656000: 0.36003705709380807\n",
      "Loss at epoch 657000: 0.35996625985474767\n",
      "Loss at epoch 658000: 0.3598956334586631\n",
      "Loss at epoch 659000: 0.3598251772582935\n",
      "Loss at epoch 660000: 0.35975489060976856\n",
      "Loss at epoch 661000: 0.35968477287258493\n",
      "Loss at epoch 662000: 0.35961482340958445\n",
      "Loss at epoch 663000: 0.35954504158693107\n",
      "Loss at epoch 664000: 0.3594754267740892\n",
      "Loss at epoch 665000: 0.3594059783438011\n",
      "Loss at epoch 666000: 0.3593366956720653\n",
      "Loss at epoch 667000: 0.3592675781381146\n",
      "Loss at epoch 668000: 0.3591986251243948\n",
      "Loss at epoch 669000: 0.35912983601654375\n",
      "Loss at epoch 670000: 0.3590612102033694\n",
      "Loss at epoch 671000: 0.3589927470768283\n",
      "Loss at epoch 672000: 0.35892444603200685\n",
      "Loss at epoch 673000: 0.35885630646709876\n",
      "Loss at epoch 674000: 0.3587883277833853\n",
      "Loss at epoch 675000: 0.3587205093852141\n",
      "Loss at epoch 676000: 0.35865285067998043\n",
      "Loss at epoch 677000: 0.35858535107810613\n",
      "Loss at epoch 678000: 0.35851800999302\n",
      "Loss at epoch 679000: 0.35845082684113816\n",
      "Loss at epoch 680000: 0.3583838010418446\n",
      "Loss at epoch 681000: 0.3583169320174714\n",
      "Loss at epoch 682000: 0.3582502191932801\n",
      "Loss at epoch 683000: 0.3581836619974422\n",
      "Loss at epoch 684000: 0.3581172598610203\n",
      "Loss at epoch 685000: 0.35805101221794944\n",
      "Loss at epoch 686000: 0.35798491850501857\n",
      "Loss at epoch 687000: 0.3579189781618518\n",
      "Loss at epoch 688000: 0.35785319063089016\n",
      "Loss at epoch 689000: 0.3577875553573733\n",
      "Loss at epoch 690000: 0.3577220717893219\n",
      "Loss at epoch 691000: 0.35765673937751913\n",
      "Loss at epoch 692000: 0.3575915575754937\n",
      "Loss at epoch 693000: 0.35752652583950173\n",
      "Loss at epoch 694000: 0.3574616436285088\n",
      "Loss at epoch 695000: 0.3573969104041738\n",
      "Loss at epoch 696000: 0.3573323256308302\n",
      "Loss at epoch 697000: 0.35726788877547083\n",
      "Loss at epoch 698000: 0.3572035993077293\n",
      "Loss at epoch 699000: 0.3571394566998641\n",
      "Loss at epoch 700000: 0.3570754604267415\n",
      "Loss at epoch 701000: 0.35701160996581893\n",
      "Loss at epoch 702000: 0.3569479047971295\n",
      "Loss at epoch 703000: 0.35688434440326433\n",
      "Loss at epoch 704000: 0.35682092826935735\n",
      "Loss at epoch 705000: 0.356757655883069\n",
      "Loss at epoch 706000: 0.35669452673456975\n",
      "Loss at epoch 707000: 0.3566315403165257\n",
      "Loss at epoch 708000: 0.35656869612408143\n",
      "Loss at epoch 709000: 0.35650599365484514\n",
      "Loss at epoch 710000: 0.35644343240887333\n",
      "Loss at epoch 711000: 0.3563810118886551\n",
      "Loss at epoch 712000: 0.35631873159909705\n",
      "Loss at epoch 713000: 0.3562565910475085\n",
      "Loss at epoch 714000: 0.3561945897435864\n",
      "Loss at epoch 715000: 0.3561327271994002\n",
      "Loss at epoch 716000: 0.35607100292937705\n",
      "Loss at epoch 717000: 0.356009416450288\n",
      "Loss at epoch 718000: 0.35594796728123246\n",
      "Loss at epoch 719000: 0.3558866549436245\n",
      "Loss at epoch 720000: 0.35582547896117783\n",
      "Loss at epoch 721000: 0.3557644388598929\n",
      "Loss at epoch 722000: 0.3557035341680411\n",
      "Loss at epoch 723000: 0.3556427644161526\n",
      "Loss at epoch 724000: 0.3555821291370005\n",
      "Loss at epoch 725000: 0.3555216278655889\n",
      "Loss at epoch 726000: 0.3554612601391381\n",
      "Loss at epoch 727000: 0.3554010254970716\n",
      "Loss at epoch 728000: 0.3553409234810021\n",
      "Loss at epoch 729000: 0.35528095363471907\n",
      "Loss at epoch 730000: 0.3552211155041744\n",
      "Loss at epoch 731000: 0.35516140863746964\n",
      "Loss at epoch 732000: 0.3551018325848438\n",
      "Loss at epoch 733000: 0.3550423868986584\n",
      "Loss at epoch 734000: 0.3549830711333869\n",
      "Loss at epoch 735000: 0.3549238848456005\n",
      "Loss at epoch 736000: 0.3548648275939557\n",
      "Loss at epoch 737000: 0.3548058989391822\n",
      "Loss at epoch 738000: 0.35474709844406904\n",
      "Loss at epoch 739000: 0.3546884256734544\n",
      "Loss at epoch 740000: 0.3546298801942114\n",
      "Loss at epoch 741000: 0.3545714615752368\n",
      "Loss at epoch 742000: 0.3545131693874385\n",
      "Loss at epoch 743000: 0.3544550032037239\n",
      "Loss at epoch 744000: 0.3543969625989874\n",
      "Loss at epoch 745000: 0.3543390471500989\n",
      "Loss at epoch 746000: 0.35428125643589226\n",
      "Loss at epoch 747000: 0.35422359003715276\n",
      "Loss at epoch 748000: 0.35416604753660624\n",
      "Loss at epoch 749000: 0.3541086285189079\n",
      "Loss at epoch 750000: 0.3540513325706295\n",
      "Loss at epoch 751000: 0.3539941592802496\n",
      "Loss at epoch 752000: 0.3539371082381409\n",
      "Loss at epoch 753000: 0.3538801790365602\n",
      "Loss at epoch 754000: 0.35382337126963626\n",
      "Loss at epoch 755000: 0.3537666845333597\n",
      "Loss at epoch 756000: 0.35371011842557154\n",
      "Loss at epoch 757000: 0.3536536725459525\n",
      "Loss at epoch 758000: 0.35359734649601166\n",
      "Loss at epoch 759000: 0.35354113987907687\n",
      "Loss at epoch 760000: 0.3534850523002832\n",
      "Loss at epoch 761000: 0.35342908336656254\n",
      "Loss at epoch 762000: 0.3533732326866333\n",
      "Loss at epoch 763000: 0.3533174998709897\n",
      "Loss at epoch 764000: 0.3532618845318922\n",
      "Loss at epoch 765000: 0.3532063862833563\n",
      "Loss at epoch 766000: 0.3531510047411425\n",
      "Loss at epoch 767000: 0.353095739522747\n",
      "Loss at epoch 768000: 0.3530405902473907\n",
      "Loss at epoch 769000: 0.35298555653600955\n",
      "Loss at epoch 770000: 0.3529306380112449\n",
      "Loss at epoch 771000: 0.35287583429743297\n",
      "Loss at epoch 772000: 0.3528211450205966\n",
      "Loss at epoch 773000: 0.35276656980843346\n",
      "Loss at epoch 774000: 0.3527121082903081\n",
      "Loss at epoch 775000: 0.3526577600972416\n",
      "Loss at epoch 776000: 0.35260352486190233\n",
      "Loss at epoch 777000: 0.3525494022185964\n",
      "Loss at epoch 778000: 0.35249539180325834\n",
      "Loss at epoch 779000: 0.35244149325344176\n",
      "Loss at epoch 780000: 0.35238770620831034\n",
      "Loss at epoch 781000: 0.35233403030862837\n",
      "Loss at epoch 782000: 0.3522804651967519\n",
      "Loss at epoch 783000: 0.3522270105166195\n",
      "Loss at epoch 784000: 0.35217366591374366\n",
      "Loss at epoch 785000: 0.35212043103520124\n",
      "Loss at epoch 786000: 0.35206730552962495\n",
      "Loss at epoch 787000: 0.35201428904719484\n",
      "Loss at epoch 788000: 0.3519613812396291\n",
      "Loss at epoch 789000: 0.3519085817601759\n",
      "Loss at epoch 790000: 0.35185589026360437\n",
      "Loss at epoch 791000: 0.3518033064061959\n",
      "Loss at epoch 792000: 0.3517508298457364\n",
      "Loss at epoch 793000: 0.3516984602415071\n",
      "Loss at epoch 794000: 0.35164619725427615\n",
      "Loss at epoch 795000: 0.3515940405462915\n",
      "Loss at epoch 796000: 0.3515419897812712\n",
      "Loss at epoch 797000: 0.3514900446243957\n",
      "Loss at epoch 798000: 0.3514382047422998\n",
      "Loss at epoch 799000: 0.3513864698030648\n",
      "Loss at epoch 800000: 0.35133483947621\n",
      "Loss at epoch 801000: 0.3512833134326848\n",
      "Loss at epoch 802000: 0.3512318913448613\n",
      "Loss at epoch 803000: 0.3511805728865254\n",
      "Loss at epoch 804000: 0.35112935773286996\n",
      "Loss at epoch 805000: 0.3510782455604869\n",
      "Loss at epoch 806000: 0.35102723604735864\n",
      "Loss at epoch 807000: 0.35097632887285163\n",
      "Loss at epoch 808000: 0.35092552371770785\n",
      "Loss at epoch 809000: 0.35087482026403766\n",
      "Loss at epoch 810000: 0.350824218195312\n",
      "Loss at epoch 811000: 0.35077371719635553\n",
      "Loss at epoch 812000: 0.35072331695333847\n",
      "Loss at epoch 813000: 0.35067301715376975\n",
      "Loss at epoch 814000: 0.35062281748648944\n",
      "Loss at epoch 815000: 0.35057271764166165\n",
      "Loss at epoch 816000: 0.3505227173107675\n",
      "Loss at epoch 817000: 0.35047281618659715\n",
      "Loss at epoch 818000: 0.3504230139632437\n",
      "Loss at epoch 819000: 0.35037331033609576\n",
      "Loss at epoch 820000: 0.35032370500182997\n",
      "Loss at epoch 821000: 0.3502741976584046\n",
      "Loss at epoch 822000: 0.35022478800505236\n",
      "Loss at epoch 823000: 0.3501754757422738\n",
      "Loss at epoch 824000: 0.3501262605718302\n",
      "Loss at epoch 825000: 0.35007714219673647\n",
      "Loss at epoch 826000: 0.35002812032125497\n",
      "Loss at epoch 827000: 0.349979194650889\n",
      "Loss at epoch 828000: 0.3499303648923753\n",
      "Loss at epoch 829000: 0.3498816307536777\n",
      "Loss at epoch 830000: 0.3498329919439815\n",
      "Loss at epoch 831000: 0.34978444817368554\n",
      "Loss at epoch 832000: 0.34973599915439646\n",
      "Loss at epoch 833000: 0.3496876445989224\n",
      "Loss at epoch 834000: 0.34963938422126606\n",
      "Loss at epoch 835000: 0.3495912177366186\n",
      "Loss at epoch 836000: 0.34954314486135374\n",
      "Loss at epoch 837000: 0.34949516531302066\n",
      "Loss at epoch 838000: 0.34944727881033866\n",
      "Loss at epoch 839000: 0.34939948507318996\n",
      "Loss at epoch 840000: 0.3493517838226148\n",
      "Loss at epoch 841000: 0.349304174780804\n",
      "Loss at epoch 842000: 0.34925665767109354\n",
      "Loss at epoch 843000: 0.3492092322179589\n",
      "Loss at epoch 844000: 0.3491618981470085\n",
      "Loss at epoch 845000: 0.34911465518497753\n",
      "Loss at epoch 846000: 0.34906750305972284\n",
      "Loss at epoch 847000: 0.34902044150021655\n",
      "Loss at epoch 848000: 0.3489734702365401\n",
      "Loss at epoch 849000: 0.3489265889998788\n",
      "Loss at epoch 850000: 0.34887979752251574\n",
      "Loss at epoch 851000: 0.3488330955378262\n",
      "Loss at epoch 852000: 0.34878648278027197\n",
      "Loss at epoch 853000: 0.34873995898539567\n",
      "Loss at epoch 854000: 0.3486935238898154\n",
      "Loss at epoch 855000: 0.34864717723121835\n",
      "Loss at epoch 856000: 0.34860091874835625\n",
      "Loss at epoch 857000: 0.3485547481810391\n",
      "Loss at epoch 858000: 0.3485086652701299\n",
      "Loss at epoch 859000: 0.34846266975753987\n",
      "Loss at epoch 860000: 0.34841676138622174\n",
      "Loss at epoch 861000: 0.3483709399001653\n",
      "Loss at epoch 862000: 0.3483252050443921\n",
      "Loss at epoch 863000: 0.3482795565649495\n",
      "Loss at epoch 864000: 0.3482339942089061\n",
      "Loss at epoch 865000: 0.3481885177243457\n",
      "Loss at epoch 866000: 0.34814312686036314\n",
      "Loss at epoch 867000: 0.3480978213670582\n",
      "Loss at epoch 868000: 0.3480526009955308\n",
      "Loss at epoch 869000: 0.3480074654978759\n",
      "Loss at epoch 870000: 0.34796241462717853\n",
      "Loss at epoch 871000: 0.3479174481375083\n",
      "Loss at epoch 872000: 0.34787256578391496\n",
      "Loss at epoch 873000: 0.34782776732242326\n",
      "Loss at epoch 874000: 0.34778305251002745\n",
      "Loss at epoch 875000: 0.3477384211046872\n",
      "Loss at epoch 876000: 0.34769387286532194\n",
      "Loss at epoch 877000: 0.3476494075518067\n",
      "Loss at epoch 878000: 0.34760502492496687\n",
      "Loss at epoch 879000: 0.34756072474657335\n",
      "Loss at epoch 880000: 0.34751650677933765\n",
      "Loss at epoch 881000: 0.34747237078690796\n",
      "Loss at epoch 882000: 0.3474283165338631\n",
      "Loss at epoch 883000: 0.34738434378570926\n",
      "Loss at epoch 884000: 0.34734045230887434\n",
      "Loss at epoch 885000: 0.3472966418707035\n",
      "Loss at epoch 886000: 0.34725291223945537\n",
      "Loss at epoch 887000: 0.3472092631842961\n",
      "Loss at epoch 888000: 0.347165694475296\n",
      "Loss at epoch 889000: 0.3471222058834242\n",
      "Loss at epoch 890000: 0.3470787971805449\n",
      "Loss at epoch 891000: 0.3470354681394123\n",
      "Loss at epoch 892000: 0.3469922185336663\n",
      "Loss at epoch 893000: 0.3469490481378285\n",
      "Loss at epoch 894000: 0.34690595672729735\n",
      "Loss at epoch 895000: 0.346862944078344\n",
      "Loss at epoch 896000: 0.3468200099681081\n",
      "Loss at epoch 897000: 0.34677715417459265\n",
      "Loss at epoch 898000: 0.3467343764766615\n",
      "Loss at epoch 899000: 0.3466916766540332\n",
      "Loss at epoch 900000: 0.3466490544872778\n",
      "Loss at epoch 901000: 0.34660650975781254\n",
      "Loss at epoch 902000: 0.34656404224789766\n",
      "Loss at epoch 903000: 0.3465216517406318\n",
      "Loss at epoch 904000: 0.34647933801994885\n",
      "Loss at epoch 905000: 0.34643710087061286\n",
      "Loss at epoch 906000: 0.34639494007821414\n",
      "Loss at epoch 907000: 0.3463528554291661\n",
      "Loss at epoch 908000: 0.3463108467106999\n",
      "Loss at epoch 909000: 0.3462689137108618\n",
      "Loss at epoch 910000: 0.34622705621850786\n",
      "Loss at epoch 911000: 0.3461852740233011\n",
      "Loss at epoch 912000: 0.34614356691570675\n",
      "Loss at epoch 913000: 0.3461019346869891\n",
      "Loss at epoch 914000: 0.34606037712920695\n",
      "Loss at epoch 915000: 0.34601889403520986\n",
      "Loss at epoch 916000: 0.34597748519863497\n",
      "Loss at epoch 917000: 0.3459361504139022\n",
      "Loss at epoch 918000: 0.34589488947621116\n",
      "Loss at epoch 919000: 0.34585370218153727\n",
      "Loss at epoch 920000: 0.34581258832662803\n",
      "Loss at epoch 921000: 0.34577154770899887\n",
      "Loss at epoch 922000: 0.34573058012692964\n",
      "Loss at epoch 923000: 0.34568968537946154\n",
      "Loss at epoch 924000: 0.345648863266393\n",
      "Loss at epoch 925000: 0.34560811358827526\n",
      "Loss at epoch 926000: 0.3455674361464102\n",
      "Loss at epoch 927000: 0.34552683074284624\n",
      "Loss at epoch 928000: 0.345486297180374\n",
      "Loss at epoch 929000: 0.34544583526252365\n",
      "Loss at epoch 930000: 0.3454054447935612\n",
      "Loss at epoch 931000: 0.3453651255784845\n",
      "Loss at epoch 932000: 0.34532487742302015\n",
      "Loss at epoch 933000: 0.3452847001336206\n",
      "Loss at epoch 934000: 0.34524459351745934\n",
      "Loss at epoch 935000: 0.3452045573824285\n",
      "Loss at epoch 936000: 0.3451645915371353\n",
      "Loss at epoch 937000: 0.34512469579089844\n",
      "Loss at epoch 938000: 0.3450848699537447\n",
      "Loss at epoch 939000: 0.34504511383640596\n",
      "Loss at epoch 940000: 0.3450054272503154\n",
      "Loss at epoch 941000: 0.34496581000760446\n",
      "Loss at epoch 942000: 0.34492626192109943\n",
      "Loss at epoch 943000: 0.3448867828043183\n",
      "Loss at epoch 944000: 0.34484737247146763\n",
      "Loss at epoch 945000: 0.3448080307374388\n",
      "Loss at epoch 946000: 0.3447687574178054\n",
      "Loss at epoch 947000: 0.3447295523288193\n",
      "Loss at epoch 948000: 0.3446904152874086\n",
      "Loss at epoch 949000: 0.34465134611117343\n",
      "Loss at epoch 950000: 0.344612344618383\n",
      "Loss at epoch 951000: 0.344573410627973\n",
      "Loss at epoch 952000: 0.3445345439595416\n",
      "Loss at epoch 953000: 0.3444957444333478\n",
      "Loss at epoch 954000: 0.3444570118703062\n",
      "Loss at epoch 955000: 0.3444183460919864\n",
      "Loss at epoch 956000: 0.344379746920608\n",
      "Loss at epoch 957000: 0.3443412141790385\n",
      "Loss at epoch 958000: 0.3443027476907905\n",
      "Loss at epoch 959000: 0.3442643472800177\n",
      "Loss at epoch 960000: 0.34422601277151305\n",
      "Loss at epoch 961000: 0.3441877439907051\n",
      "Loss at epoch 962000: 0.34414954076365556\n",
      "Loss at epoch 963000: 0.34411140291705566\n",
      "Loss at epoch 964000: 0.344073330278224\n",
      "Loss at epoch 965000: 0.34403532267510334\n",
      "Loss at epoch 966000: 0.3439973799362577\n",
      "Loss at epoch 967000: 0.34395950189086943\n",
      "Loss at epoch 968000: 0.34392168836873693\n",
      "Loss at epoch 969000: 0.3438839392002707\n",
      "Loss at epoch 970000: 0.3438462542164917\n",
      "Loss at epoch 971000: 0.34380863324902794\n",
      "Loss at epoch 972000: 0.3437710761301119\n",
      "Loss at epoch 973000: 0.3437335826925776\n",
      "Loss at epoch 974000: 0.34369615276985815\n",
      "Loss at epoch 975000: 0.34365878619598283\n",
      "Loss at epoch 976000: 0.3436214828055742\n",
      "Loss at epoch 977000: 0.3435842424338456\n",
      "Loss at epoch 978000: 0.34354706491659853\n",
      "Loss at epoch 979000: 0.3435099500902201\n",
      "Loss at epoch 980000: 0.3434728977916798\n",
      "Loss at epoch 981000: 0.34343590785852773\n",
      "Loss at epoch 982000: 0.34339898012889086\n",
      "Loss at epoch 983000: 0.3433621144414713\n",
      "Loss at epoch 984000: 0.343325310635544\n",
      "Loss at epoch 985000: 0.34328856855095274\n",
      "Loss at epoch 986000: 0.3432518880281091\n",
      "Loss at epoch 987000: 0.3432152689079889\n",
      "Loss at epoch 988000: 0.34317871103212993\n",
      "Loss at epoch 989000: 0.3431422142426299\n",
      "Loss at epoch 990000: 0.34310577838214323\n",
      "Loss at epoch 991000: 0.3430694032938788\n",
      "Loss at epoch 992000: 0.34303308882159783\n",
      "Loss at epoch 993000: 0.3429968348096108\n",
      "Loss at epoch 994000: 0.34296064110277535\n",
      "Loss at epoch 995000: 0.34292450754649395\n",
      "Loss at epoch 996000: 0.3428884339867112\n",
      "Loss at epoch 997000: 0.34285242026991136\n",
      "Loss at epoch 998000: 0.34281646624311635\n",
      "Loss at epoch 999000: 0.3427805717538828\n"
     ]
    }
   ],
   "source": [
    "W = logistic_regression(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ln4d4w7Kji9",
    "outputId": "09f9340c-5bd6-43cb-cf3e-1719f8373406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code train accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train_bar = np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis = 1)\n",
    "y_pred = sigmoid(X_train_bar,W)\n",
    "y_pred = np.where(y_pred>0.5,1,0)\n",
    "print(\"Code train accuracy: {}\".format(np.mean(y_train==y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RecvJH2sKiQG",
    "outputId": "dfa3a3e7-8c93-462e-e262-d415375bdb1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code test accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "X_test_bar = np.concatenate([np.ones([X_test.shape[0],1]),X_test],axis = 1)\n",
    "y_pred = sigmoid(X_test_bar,W)\n",
    "y_pred = np.where(y_pred>0.5,1,0)\n",
    "print(\"Code test accuracy: {}\".format(np.mean(y_test==y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xjs76SqO0hlM",
    "outputId": "40834560-8a51-495d-e70a-79ecd58a3971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn train accuracy: 0.8620689655172413\n",
      "Sklearn test accoracy: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "print(\"Sklearn train accuracy: {}\".format(clf.score(X_train,y_train)))\n",
    "print(\"Sklearn test accoracy: {}\".format(clf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhk-P8J861am"
   },
   "source": [
    "#**Câu 2:**\n",
    "Hãy xây dựng mô hình softmax regression trên bộ Iris (nên Normalize data), so sánh với thư viện sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnknnN9v3EUW"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "\n",
    "max=np.max(X_train,axis=0)\n",
    "min=np.min(X_train,axis=0)\n",
    "\n",
    "X_train=(X_train-min)/(max-min)\n",
    "X_test=(X_test-min)/(max-min)\n",
    "\n",
    "\n",
    "y_train_onehot = np.zeros( (y_train.size, y_train.max() + 1),dtype=int)\n",
    "y_train_onehot[np.arange(y_train.size), y_train.reshape(-1)] = 1\n",
    "\n",
    "y_test_onehot = np.zeros( (y_test.size, y_test.max() + 1) ,dtype=int)\n",
    "y_test_onehot[np.arange(y_test.size), y_test.reshape(-1)] = 1\n",
    "\n",
    "\n",
    "X_train_bar=np.concatenate([np.ones([X_train.shape[0],1]),X_train],axis=1)\n",
    "X_test_bar=np.concatenate([np.ones([X_test.shape[0],1]),X_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSS3twPF7RhP"
   },
   "outputs": [],
   "source": [
    "def softmax(X,y):\n",
    "  print(\"n: {}\".format(X.shape[0]))\n",
    "  print(\"m: {}\".format(X.shape[1]+1))\n",
    "  print(\"C: {}\".format(y.shape[1]))\n",
    "  lr = 0.0001\n",
    "  Xbar = np.concatenate([np.ones([X.shape[0],1]),X],axis = 1)# nxm\n",
    "  W = np.zeros([Xbar.shape[1],y.shape[1]])# mxC\n",
    "  preLoss = 1e9\n",
    "  iStop = -1\n",
    "  for i in range(50000):\n",
    "    e_XW = np.exp(np.matmul(Xbar,W))# nxC\n",
    "    sum_e_XW = np.sum(e_XW,axis = 1,dtype=np.float64).reshape(X.shape[0],1)# nxC -> nx1\n",
    "    gradient = np.matmul(Xbar.T,y+e_XW/sum_e_XW)# mxC\n",
    "    gradient = gradient.reshape(Xbar.shape[1],y.shape[1])\n",
    "    W = W - lr*gradient\n",
    "    sum_e_XW_1 = np.matmul(sum_e_XW,np.ones([1,y.shape[1]]))# nx1 -> nxC\n",
    "    temp = np.matmul((-np.log(sum_e_XW_1)+np.matmul(Xbar,W)).T,y)# CxC\n",
    "    #loss = 1/Xbar[0]*np.sum(temp.ravel())# number\n",
    "    loss = np.sum(temp.ravel(),dtype=np.float64)# number\n",
    "    loss = 1/Xbar.shape[0]*loss\n",
    "    if (i%1000==0):\n",
    "      print(\"Loss at epoch {}: {}\".format(i, loss))\n",
    "    # Nếu 1000 vòng lặp không thay đổi quá nhiều thì thoát\n",
    "    if (np.abs(loss-preLoss)<=1e-5):\n",
    "      if (i-iStop>=1000):\n",
    "        break\n",
    "    else:\n",
    "      iStop = i\n",
    "      preLoss = loss\n",
    "  return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4z3CK0uZVUR-",
    "outputId": "6db9ffff-7560-45b5-c32c-433cd4142c5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: 100\n",
      "m: 5\n",
      "C: 3\n",
      "Loss at epoch 0: -3.333083765258848\n",
      "Loss at epoch 1000: -3.634893631759327\n",
      "Loss at epoch 2000: -3.978079817175991\n",
      "Loss at epoch 3000: -4.3096258151752656\n",
      "Loss at epoch 4000: -4.6234234106673044\n",
      "Loss at epoch 5000: -4.924379618781933\n",
      "Loss at epoch 6000: -5.216715143984261\n",
      "Loss at epoch 7000: -5.503120239417098\n",
      "Loss at epoch 8000: -5.785195703397447\n",
      "Loss at epoch 9000: -6.063878741475906\n",
      "Loss at epoch 10000: -6.3397222456240385\n",
      "Loss at epoch 11000: -6.613061454578057\n",
      "Loss at epoch 12000: -6.884109983670207\n",
      "Loss at epoch 13000: -7.153014235398672\n",
      "Loss at epoch 14000: -7.4198838765657245\n",
      "Loss at epoch 15000: -7.684808704598788\n",
      "Loss at epoch 16000: -7.9478678258389825\n",
      "Loss at epoch 17000: -8.209134520848906\n",
      "Loss at epoch 18000: -8.46867871411827\n",
      "Loss at epoch 19000: -8.726568134714126\n",
      "Loss at epoch 20000: -8.98286878107068\n",
      "Loss at epoch 21000: -9.237645033300739\n",
      "Loss at epoch 22000: -9.49095960265673\n",
      "Loss at epoch 23000: -9.742873420347674\n",
      "Loss at epoch 24000: -9.993445518503373\n",
      "Loss at epoch 25000: -10.242732928459843\n",
      "Loss at epoch 26000: -10.4907906064073\n",
      "Loss at epoch 27000: -10.737671388435762\n",
      "Loss at epoch 28000: -10.983425973042172\n",
      "Loss at epoch 29000: -11.228102927425383\n",
      "Loss at epoch 30000: -11.47174871338254\n",
      "Loss at epoch 31000: -11.714407728722792\n",
      "Loss at epoch 32000: -11.956122360520114\n",
      "Loss at epoch 33000: -12.196933047034014\n",
      "Loss at epoch 34000: -12.436878345654291\n",
      "Loss at epoch 35000: -12.6759950047131\n",
      "Loss at epoch 36000: -12.914318037435766\n",
      "Loss at epoch 37000: -13.151880796677606\n",
      "Loss at epoch 38000: -13.388715049391935\n",
      "Loss at epoch 39000: -13.624851050038346\n",
      "Loss at epoch 40000: -13.860317612334379\n",
      "Loss at epoch 41000: -14.09513284184204\n",
      "Loss at epoch 42000: nan\n",
      "Loss at epoch 43000: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 44000: nan\n",
      "Loss at epoch 45000: nan\n",
      "Loss at epoch 46000: nan\n",
      "Loss at epoch 47000: nan\n",
      "Loss at epoch 48000: nan\n",
      "Loss at epoch 49000: nan\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-64e4b2bcb58d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# mxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mXbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatnate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# nxm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0me_XW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# nxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msum_e_XW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_XW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# nxC -> nx1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me_XW\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum_e_XW\u001b[0m \u001b[0;31m# nxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 raise AttributeError(\"module {!r} has no attribute \"\n\u001b[0;32m--> 215\u001b[0;31m                                      \"{!r}\".format(__name__, attr))\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'concatnate'"
     ]
    }
   ],
   "source": [
    "W = softmax(X_train,y_train_onehot)# mxC\n",
    "Xbar = np.concatnate([np.ones([X_train.shape[0],1]),X_train])# nxm\n",
    "e_XW = np.exp(np.matmul(Xbar,W))# nxC\n",
    "sum_e_XW = np.sum(e_XW,axis = 1)# nxC -> nx1\n",
    "y_pred = e_XW / sum_e_XW # nxC\n",
    "print(np.mean(y_test==np.argmax(y_pred, axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_oxZ4amXWBb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NMMH_TH3_MSSV.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
